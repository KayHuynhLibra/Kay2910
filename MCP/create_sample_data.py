#!/usr/bin/env python3
"""
Create Sample Data - T·∫°o d·ªØ li·ªáu m·∫´u th·ª±c t·∫ø cho d·ª± √°n AI 2025 v·ªõi MCP
"""

import os
import json
import numpy as np
import torch
import pandas as pd
from datetime import datetime
import cv2
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import seaborn as sns

def create_directories():
    """T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt"""
    directories = [
        'data/raw',
        'data/processed', 
        'data/samples',
        'datasets/text',
        'datasets/images',
        'datasets/numerical',
        'models/checkpoints',
        'models/saved',
        'logs/training',
        'logs/evaluation',
        'outputs/plots',
        'outputs/results',
        'experiments/configs',
        'experiments/results'
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"Created: {directory}")

def create_text_data():
    """T·∫°o d·ªØ li·ªáu text m·∫´u"""
    print("\n=== T·∫°o d·ªØ li·ªáu text ===")
    
    # D·ªØ li·ªáu vƒÉn b·∫£n ƒëa d·∫°ng
    text_samples = {
        'sentiment': [
            "T√¥i r·∫•t h√†i l√≤ng v·ªõi s·∫£n ph·∫©m n√†y!",
            "Ch·∫•t l∆∞·ª£ng kh√¥ng t·ªët nh∆∞ mong ƒë·ª£i.",
            "D·ªãch v·ª• kh√°ch h√†ng tuy·ªát v·ªùi!",
            "Gi√° c·∫£ h·ª£p l√Ω v√† ch·∫•t l∆∞·ª£ng t·ªët.",
            "Kh√¥ng n√™n mua s·∫£n ph·∫©m n√†y.",
            "R·∫•t th√≠ch c√°ch thi·∫øt k·∫ø c·ªßa s·∫£n ph·∫©m.",
            "Giao h√†ng ch·∫≠m v√† kh√¥ng ƒë√∫ng h·∫πn.",
            "S·∫£n ph·∫©m ƒë√∫ng nh∆∞ m√¥ t·∫£, r·∫•t h√†i l√≤ng!"
        ],
        'news': [
            "AI ƒëang thay ƒë·ªïi c√°ch ch√∫ng ta l√†m vi·ªác trong nƒÉm 2025.",
            "C√¥ng ngh·ªá blockchain ƒë∆∞·ª£c ·ª©ng d·ª•ng r·ªông r√£i trong t√†i ch√≠nh.",
            "Vi·ªát Nam ƒë·∫°t ƒë∆∞·ª£c nhi·ªÅu th√†nh t·ª±u trong lƒ©nh v·ª±c c√¥ng ngh·ªá.",
            "Bi·∫øn ƒë·ªïi kh√≠ h·∫≠u ·∫£nh h∆∞·ªüng nghi√™m tr·ªçng ƒë·∫øn n√¥ng nghi·ªáp.",
            "Gi√°o d·ª•c tr·ª±c tuy·∫øn tr·ªü th√†nh xu h∆∞·ªõng ch√≠nh sau ƒë·∫°i d·ªãch.",
            "Th√†nh ph·ªë th√¥ng minh ƒëang ƒë∆∞·ª£c ph√°t tri·ªÉn t·∫°i nhi·ªÅu n∆°i.",
            "Y t·∫ø s·ªë gi√∫p c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng chƒÉm s√≥c s·ª©c kh·ªèe.",
            "NƒÉng l∆∞·ª£ng t√°i t·∫°o chi·∫øm ∆∞u th·∫ø trong t∆∞∆°ng lai."
        ],
        'technical': [
            "Transformer architecture revolutionized natural language processing.",
            "Deep learning models require large amounts of training data.",
            "Federated learning enables privacy-preserving machine learning.",
            "Multimodal AI combines text, image, and audio processing.",
            "Edge computing reduces latency for real-time applications.",
            "Quantum computing shows promise for complex optimization problems.",
            "Computer vision algorithms can detect objects in real-time.",
            "Reinforcement learning is used in autonomous systems."
        ]
    }
    
    # L∆∞u v√†o file
    for category, texts in text_samples.items():
        filename = f'datasets/text/{category}_data.json'
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'category': category,
                'samples': texts,
                'created_at': datetime.now().isoformat(),
                'total_samples': len(texts)
            }, f, ensure_ascii=False, indent=2)
        print(f"Created: {filename} ({len(texts)} samples)")
    
    # T·∫°o file t·ªïng h·ª£p
    all_texts = []
    for texts in text_samples.values():
        all_texts.extend(texts)
    
    with open('datasets/text/all_text_data.json', 'w', encoding='utf-8') as f:
        json.dump({
            'all_samples': all_texts,
            'created_at': datetime.now().isoformat(),
            'total_samples': len(all_texts)
        }, f, ensure_ascii=False, indent=2)
    print(f"Created: datasets/text/all_text_data.json ({len(all_texts)} samples)")

def create_image_data():
    """T·∫°o d·ªØ li·ªáu ·∫£nh m·∫´u"""
    print("\n=== T·∫°o d·ªØ li·ªáu ·∫£nh ===")
    
    # T·∫°o ·∫£nh m·∫´u v·ªõi c√°c h√¨nh d·∫°ng kh√°c nhau
    image_categories = {
        'shapes': ['circle', 'square', 'triangle', 'rectangle'],
        'colors': ['red', 'green', 'blue', 'yellow'],
        'patterns': ['stripes', 'dots', 'solid', 'gradient']
    }
    
    for category, items in image_categories.items():
        os.makedirs(f'datasets/images/{category}', exist_ok=True)
        
        for i, item in enumerate(items):
            # T·∫°o ·∫£nh 224x224 v·ªõi m√†u s·∫Øc kh√°c nhau
            img = Image.new('RGB', (224, 224), color='white')
            draw = ImageDraw.Draw(img)
            
            # V·∫Ω h√¨nh d·∫°ng kh√°c nhau
            if category == 'shapes':
                if item == 'circle':
                    draw.ellipse([50, 50, 174, 174], fill='red', outline='black', width=3)
                elif item == 'square':
                    draw.rectangle([50, 50, 174, 174], fill='blue', outline='black', width=3)
                elif item == 'triangle':
                    draw.polygon([(112, 50), (50, 174), (174, 174)], fill='green', outline='black', width=3)
                elif item == 'rectangle':
                    draw.rectangle([30, 80, 194, 144], fill='yellow', outline='black', width=3)
            
            # L∆∞u ·∫£nh
            filename = f'datasets/images/{category}/{item}.png'
            img.save(filename)
            print(f"Created: {filename}")
    
    # T·∫°o metadata
    metadata = {
        'categories': image_categories,
        'image_size': (224, 224),
        'format': 'PNG',
        'created_at': datetime.now().isoformat()
    }
    
    with open('datasets/images/metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    print("Created: datasets/images/metadata.json")

def create_numerical_data():
    """T·∫°o d·ªØ li·ªáu s·ªë m·∫´u"""
    print("\n=== T·∫°o d·ªØ li·ªáu s·ªë ===")
    
    # T·∫°o d·ªØ li·ªáu v·ªõi c√°c ph√¢n ph·ªëi kh√°c nhau
    np.random.seed(42)  # ƒê·ªÉ k·∫øt qu·∫£ c√≥ th·ªÉ t√°i t·∫°o
    
    datasets = {
        'sales_data': {
            'data': np.random.normal(1000, 200, (1000, 5)),  # Doanh s·ªë
            'columns': ['product_id', 'quantity', 'price', 'customer_id', 'date_code']
        },
        'user_behavior': {
            'data': np.random.exponential(2, (500, 4)),  # Th·ªùi gian s·ª≠ d·ª•ng
            'columns': ['session_duration', 'page_views', 'clicks', 'bounce_rate']
        },
        'sensor_data': {
            'data': np.random.uniform(20, 30, (800, 6)),  # D·ªØ li·ªáu c·∫£m bi·∫øn
            'columns': ['temperature', 'humidity', 'pressure', 'light', 'noise', 'motion']
        },
        'financial_data': {
            'data': np.random.lognormal(0, 1, (600, 4)),  # D·ªØ li·ªáu t√†i ch√≠nh
            'columns': ['stock_price', 'volume', 'market_cap', 'pe_ratio']
        }
    }
    
    for name, dataset in datasets.items():
        # T·∫°o DataFrame
        df = pd.DataFrame(dataset['data'], columns=dataset['columns'])
        
        # Th√™m ID
        df.insert(0, 'id', range(1, len(df) + 1))
        
        # L∆∞u CSV
        csv_filename = f'datasets/numerical/{name}.csv'
        df.to_csv(csv_filename, index=False)
        print(f"Created: {csv_filename} ({len(df)} rows, {len(df.columns)} columns)")
        
        # L∆∞u JSON
        json_filename = f'datasets/numerical/{name}.json'
        with open(json_filename, 'w') as f:
            json.dump({
                'name': name,
                'data': df.to_dict('records'),
                'statistics': {
                    'mean': df.mean().to_dict(),
                    'std': df.std().to_dict(),
                    'min': df.min().to_dict(),
                    'max': df.max().to_dict()
                },
                'created_at': datetime.now().isoformat()
            }, f, indent=2)
        print(f"Created: {json_filename}")
    
    # T·∫°o file t·ªïng h·ª£p
    all_data = {}
    for name, dataset in datasets.items():
        all_data[name] = {
            'shape': dataset['data'].shape,
            'columns': dataset['columns'],
            'mean': dataset['data'].mean(axis=0).tolist(),
            'std': dataset['data'].std(axis=0).tolist()
        }
    
    with open('datasets/numerical/summary.json', 'w') as f:
        json.dump({
            'datasets': all_data,
            'total_datasets': len(datasets),
            'created_at': datetime.now().isoformat()
        }, f, indent=2)
    print("Created: datasets/numerical/summary.json")

def create_config_files():
    """T·∫°o file c·∫•u h√¨nh cho c√°c th√≠ nghi·ªám"""
    print("\n=== T·∫°o file c·∫•u h√¨nh ===")
    
    configs = {
        'mcp_config.json': {
            'server': {
                'host': 'localhost',
                'port': 8000,
                'max_connections': 10
            },
            'resources': {
                'database': 'sqlite:///data/samples/sample.db',
                'filesystem': './data/samples',
                'api_endpoint': 'https://api.example.com'
            },
            'tools': {
                'data_analysis': True,
                'file_processor': True,
                'model_training': True
            }
        },
        'training_config.json': {
            'model': {
                'type': 'transformer',
                'hidden_size': 768,
                'num_layers': 12,
                'num_heads': 12
            },
            'training': {
                'batch_size': 32,
                'learning_rate': 1e-4,
                'num_epochs': 100,
                'warmup_steps': 1000
            },
            'data': {
                'train_split': 0.8,
                'val_split': 0.1,
                'test_split': 0.1,
                'max_length': 512
            }
        },
        'multimodal_config.json': {
            'text_encoder': {
                'model_name': 'bert-base-uncased',
                'max_length': 512,
                'hidden_size': 768
            },
            'image_encoder': {
                'model_name': 'resnet50',
                'input_size': 224,
                'hidden_size': 512
            },
            'fusion': {
                'method': 'attention',
                'hidden_size': 1024,
                'dropout': 0.1
            }
        }
    }
    
    for filename, config in configs.items():
        filepath = f'experiments/configs/{filename}'
        with open(filepath, 'w') as f:
            json.dump(config, f, indent=2)
        print(f"Created: {filepath}")

def create_sample_plots():
    """T·∫°o c√°c bi·ªÉu ƒë·ªì m·∫´u"""
    print("\n=== T·∫°o bi·ªÉu ƒë·ªì m·∫´u ===")
    
    # T·∫°o d·ªØ li·ªáu cho bi·ªÉu ƒë·ªì
    x = np.linspace(0, 10, 100)
    y1 = np.sin(x)
    y2 = np.cos(x)
    y3 = np.exp(-x/3)
    
    # 1. Line plot
    plt.figure(figsize=(10, 6))
    plt.plot(x, y1, label='sin(x)', linewidth=2)
    plt.plot(x, y2, label='cos(x)', linewidth=2)
    plt.plot(x, y3, label='exp(-x/3)', linewidth=2)
    plt.title('Sample Mathematical Functions')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig('outputs/plots/mathematical_functions.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("Created: outputs/plots/mathematical_functions.png")
    
    # 2. Scatter plot
    np.random.seed(42)
    x_scatter = np.random.normal(0, 1, 100)
    y_scatter = 2 * x_scatter + np.random.normal(0, 0.5, 100)
    
    plt.figure(figsize=(8, 6))
    plt.scatter(x_scatter, y_scatter, alpha=0.6, color='blue')
    plt.title('Sample Scatter Plot')
    plt.xlabel('X values')
    plt.ylabel('Y values')
    plt.grid(True, alpha=0.3)
    plt.savefig('outputs/plots/scatter_plot.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("Created: outputs/plots/scatter_plot.png")
    
    # 3. Histogram
    data_hist = np.random.normal(0, 1, 1000)
    
    plt.figure(figsize=(8, 6))
    plt.hist(data_hist, bins=30, alpha=0.7, color='green', edgecolor='black')
    plt.title('Sample Histogram (Normal Distribution)')
    plt.xlabel('Values')
    plt.ylabel('Frequency')
    plt.grid(True, alpha=0.3)
    plt.savefig('outputs/plots/histogram.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("Created: outputs/plots/histogram.png")

def create_log_files():
    """T·∫°o file log m·∫´u"""
    print("\n=== T·∫°o file log m·∫´u ===")
    
    # Training log
    training_log = """2025-01-15 10:00:00 - INFO - Starting training session
2025-01-15 10:00:05 - INFO - Loading dataset: 1000 samples
2025-01-15 10:00:10 - INFO - Model initialized with 1.2M parameters
2025-01-15 10:01:00 - INFO - Epoch 1/100 - Loss: 2.3456 - Accuracy: 0.2345
2025-01-15 10:02:00 - INFO - Epoch 2/100 - Loss: 1.9876 - Accuracy: 0.3456
2025-01-15 10:03:00 - INFO - Epoch 3/100 - Loss: 1.6543 - Accuracy: 0.4567
2025-01-15 10:04:00 - INFO - Epoch 4/100 - Loss: 1.4321 - Accuracy: 0.5678
2025-01-15 10:05:00 - INFO - Epoch 5/100 - Loss: 1.2345 - Accuracy: 0.6789
"""
    
    with open('logs/training/sample_training.log', 'w') as f:
        f.write(training_log)
    print("Created: logs/training/sample_training.log")
    
    # Evaluation log
    eval_log = """2025-01-15 11:00:00 - INFO - Starting evaluation
2025-01-15 11:00:05 - INFO - Loading test dataset: 200 samples
2025-01-15 11:00:10 - INFO - Running inference...
2025-01-15 11:00:30 - INFO - Evaluation completed
2025-01-15 11:00:30 - INFO - Test Accuracy: 0.8234
2025-01-15 11:00:30 - INFO - Test Loss: 0.4567
2025-01-15 11:00:30 - INFO - Precision: 0.8123
2025-01-15 11:00:30 - INFO - Recall: 0.7890
2025-01-15 11:00:30 - INFO - F1-Score: 0.8005
"""
    
    with open('logs/evaluation/sample_evaluation.log', 'w') as f:
        f.write(eval_log)
    print("Created: logs/evaluation/sample_evaluation.log")

def main():
    """H√†m ch√≠nh ƒë·ªÉ t·∫°o t·∫•t c·∫£ d·ªØ li·ªáu m·∫´u"""
    print("=== T·∫°o d·ªØ li·ªáu m·∫´u cho d·ª± √°n AI 2025 v·ªõi MCP ===")
    print("=" * 60)
    
    # T·∫°o th∆∞ m·ª•c
    create_directories()
    
    # T·∫°o d·ªØ li·ªáu
    create_text_data()
    create_image_data()
    create_numerical_data()
    
    # T·∫°o file c·∫•u h√¨nh
    create_config_files()
    
    # T·∫°o bi·ªÉu ƒë·ªì m·∫´u
    create_sample_plots()
    
    # T·∫°o file log
    create_log_files()
    
    print("\n" + "=" * 60)
    print("‚úÖ Ho√†n th√†nh t·∫°o d·ªØ li·ªáu m·∫´u!")
    print("\nüìÅ C·∫•u tr√∫c d·ªØ li·ªáu ƒë√£ t·∫°o:")
    print("‚îú‚îÄ‚îÄ data/")
    print("‚îÇ   ‚îú‚îÄ‚îÄ raw/")
    print("‚îÇ   ‚îú‚îÄ‚îÄ processed/")
    print("‚îÇ   ‚îî‚îÄ‚îÄ samples/")
    print("‚îú‚îÄ‚îÄ datasets/")
    print("‚îÇ   ‚îú‚îÄ‚îÄ text/ (24 samples)")
    print("‚îÇ   ‚îú‚îÄ‚îÄ images/ (16 images)")
    print("‚îÇ   ‚îî‚îÄ‚îÄ numerical/ (4 datasets)")
    print("‚îú‚îÄ‚îÄ models/")
    print("‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/")
    print("‚îÇ   ‚îî‚îÄ‚îÄ saved/")
    print("‚îú‚îÄ‚îÄ logs/")
    print("‚îÇ   ‚îú‚îÄ‚îÄ training/")
    print("‚îÇ   ‚îî‚îÄ‚îÄ evaluation/")
    print("‚îú‚îÄ‚îÄ outputs/")
    print("‚îÇ   ‚îú‚îÄ‚îÄ plots/ (3 plots)")
    print("‚îÇ   ‚îî‚îÄ‚îÄ results/")
    print("‚îî‚îÄ‚îÄ experiments/")
    print("    ‚îú‚îÄ‚îÄ configs/ (3 configs)")
    print("    ‚îî‚îÄ‚îÄ results/")
    
    print("\nüöÄ B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu th·ª≠ nghi·ªám v·ªõi d·ªØ li·ªáu n√†y!")

if __name__ == "__main__":
    main() 