#!/usr/bin/env python3
"""
Deploy Experiments - Tri·ªÉn khai th·ª≠ nghi·ªám v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø
"""

import os
import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from PIL import Image
import matplotlib.pyplot as plt
from datetime import datetime
import logging
from typing import Dict, List, Any

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataLoader:
    """Class ƒë·ªÉ load d·ªØ li·ªáu t·ª´ c√°c folder"""
    
    def __init__(self, base_path='.'):
        self.base_path = base_path
    
    def load_text_data(self, category='all'):
        """Load d·ªØ li·ªáu text"""
        if category == 'all':
            filepath = f'{self.base_path}/datasets/text/all_text_data.json'
        else:
            filepath = f'{self.base_path}/datasets/text/{category}_data.json'
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"Loaded {len(data.get('all_samples', data.get('samples', [])))} text samples from {category}")
        return data
    
    def load_numerical_data(self, dataset_name):
        """Load d·ªØ li·ªáu s·ªë"""
        filepath = f'{self.base_path}/datasets/numerical/{dataset_name}.csv'
        df = pd.read_csv(filepath)
        
        logger.info(f"Loaded {len(df)} rows, {len(df.columns)} columns from {dataset_name}")
        return df
    
    def load_image_data(self, category='shapes'):
        """Load d·ªØ li·ªáu ·∫£nh"""
        folder_path = f'{self.base_path}/datasets/images/{category}'
        images = []
        filenames = []
        
        for filename in os.listdir(folder_path):
            if filename.endswith('.png'):
                filepath = os.path.join(folder_path, filename)
                img = Image.open(filepath)
                images.append(img)
                filenames.append(filename)
        
        logger.info(f"Loaded {len(images)} images from {category}")
        return images, filenames
    
    def load_config(self, config_name):
        """Load file c·∫•u h√¨nh"""
        filepath = f'{self.base_path}/experiments/configs/{config_name}'
        with open(filepath, 'r') as f:
            config = json.load(f)
        
        logger.info(f"Loaded config: {config_name}")
        return config

class TextProcessor:
    """X·ª≠ l√Ω d·ªØ li·ªáu text"""
    
    def __init__(self):
        self.vocab = set()
        self.word_to_idx = {}
        self.idx_to_word = {}
    
    def build_vocab(self, texts: List[str]):
        """X√¢y d·ª±ng vocabulary"""
        for text in texts:
            words = text.lower().split()
            self.vocab.update(words)
        
        for idx, word in enumerate(sorted(self.vocab)):
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
        
        logger.info(f"Built vocabulary with {len(self.vocab)} words")
    
    def text_to_sequence(self, text: str, max_length: int = 50):
        """Chuy·ªÉn text th√†nh sequence"""
        words = text.lower().split()
        sequence = [self.word_to_idx.get(word, 0) for word in words[:max_length]]
        
        # Padding
        while len(sequence) < max_length:
            sequence.append(0)
        
        return sequence
    
    def analyze_sentiment(self, texts: List[str]):
        """Ph√¢n t√≠ch sentiment ƒë∆°n gi·∫£n"""
        positive_words = ['t·ªët', 'tuy·ªát', 'h√†i', 'th√≠ch', 'r·∫•t', 'ƒë√∫ng', 'ch·∫•t']
        negative_words = ['kh√¥ng', 'x·∫•u', 'ch·∫≠m', 'sai', 'k√©m', 't·ªá']
        
        results = []
        for text in texts:
            text_lower = text.lower()
            pos_count = sum(1 for word in positive_words if word in text_lower)
            neg_count = sum(1 for word in negative_words if word in text_lower)
            
            if pos_count > neg_count:
                sentiment = 'positive'
            elif neg_count > pos_count:
                sentiment = 'negative'
            else:
                sentiment = 'neutral'
            
            results.append({
                'text': text,
                'sentiment': sentiment,
                'positive_score': pos_count,
                'negative_score': neg_count
            })
        
        return results

class NumericalProcessor:
    """X·ª≠ l√Ω d·ªØ li·ªáu s·ªë"""
    
    def __init__(self):
        pass
    
    def analyze_dataset(self, df: pd.DataFrame):
        """Ph√¢n t√≠ch dataset"""
        analysis = {
            'shape': df.shape,
            'columns': list(df.columns),
            'dtypes': df.dtypes.to_dict(),
            'missing_values': df.isnull().sum().to_dict(),
            'statistics': {}
        }
        
        # Th·ªëng k√™ cho c√°c c·ªôt s·ªë
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            analysis['statistics'][col] = {
                'mean': df[col].mean(),
                'std': df[col].std(),
                'min': df[col].min(),
                'max': df[col].max(),
                'median': df[col].median()
            }
        
        return analysis
    
    def create_correlations(self, df: pd.DataFrame):
        """T·∫°o ma tr·∫≠n correlation"""
        numeric_df = df.select_dtypes(include=[np.number])
        return numeric_df.corr()
    
    def detect_outliers(self, df: pd.DataFrame, columns=None):
        """Ph√°t hi·ªán outliers"""
        if columns is None:
            columns = df.select_dtypes(include=[np.number]).columns
        
        outliers = {}
        for col in columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers[col] = {
                'lower_bound': lower_bound,
                'upper_bound': upper_bound,
                'outlier_count': len(df[(df[col] < lower_bound) | (df[col] > upper_bound)])
            }
        
        return outliers

class ImageProcessor:
    """X·ª≠ l√Ω d·ªØ li·ªáu ·∫£nh"""
    
    def __init__(self):
        pass
    
    def analyze_images(self, images: List[Image.Image]):
        """Ph√¢n t√≠ch ·∫£nh"""
        analysis = []
        
        for i, img in enumerate(images):
            # Chuy·ªÉn sang numpy array
            img_array = np.array(img)
            
            # Th·ªëng k√™ c∆° b·∫£n
            stats = {
                'index': i,
                'size': img.size,
                'mode': img.mode,
                'shape': img_array.shape,
                'mean_rgb': img_array.mean(axis=(0, 1)).tolist(),
                'std_rgb': img_array.std(axis=(0, 1)).tolist(),
                'min_rgb': img_array.min(axis=(0, 1)).tolist(),
                'max_rgb': img_array.max(axis=(0, 1)).tolist()
            }
            
            analysis.append(stats)
        
        return analysis
    
    def create_image_grid(self, images: List[Image.Image], filenames: List[str], 
                         save_path='outputs/plots/image_grid.png'):
        """T·∫°o grid ·∫£nh"""
        n_images = len(images)
        cols = min(4, n_images)
        rows = (n_images + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(15, 3*rows))
        if rows == 1:
            axes = [axes] if cols == 1 else axes
        else:
            axes = axes.flatten()
        
        for i, (img, filename) in enumerate(zip(images, filenames)):
            if i < len(axes):
                axes[i].imshow(img)
                axes[i].set_title(filename, fontsize=10)
                axes[i].axis('off')
        
        # ·∫®n c√°c subplot kh√¥ng s·ª≠ d·ª•ng
        for i in range(n_images, len(axes)):
            axes[i].axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Created image grid: {save_path}")

class ExperimentRunner:
    """Ch·∫°y c√°c th√≠ nghi·ªám"""
    
    def __init__(self):
        self.data_loader = DataLoader()
        self.text_processor = TextProcessor()
        self.numerical_processor = NumericalProcessor()
        self.image_processor = ImageProcessor()
        self.results = {}
    
    def run_text_experiment(self):
        """Ch·∫°y th√≠ nghi·ªám v·ªõi d·ªØ li·ªáu text"""
        logger.info("=== Running Text Experiment ===")
        
        # Load d·ªØ li·ªáu
        text_data = self.data_loader.load_text_data('all')
        texts = text_data['all_samples']
        
        # X√¢y d·ª±ng vocabulary
        self.text_processor.build_vocab(texts)
        
        # Ph√¢n t√≠ch sentiment
        sentiment_results = self.text_processor.analyze_sentiment(texts)
        
        # Th·ªëng k√™
        sentiment_counts = {}
        for result in sentiment_results:
            sentiment = result['sentiment']
            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
        
        # L∆∞u k·∫øt qu·∫£
        self.results['text_experiment'] = {
            'total_samples': len(texts),
            'vocabulary_size': len(self.text_processor.vocab),
            'sentiment_analysis': sentiment_results,
            'sentiment_distribution': sentiment_counts
        }
        
        logger.info(f"Text experiment completed: {len(texts)} samples, {len(self.text_processor.vocab)} vocab")
        return self.results['text_experiment']
    
    def run_numerical_experiment(self, dataset_name='sales_data'):
        """Ch·∫°y th√≠ nghi·ªám v·ªõi d·ªØ li·ªáu s·ªë"""
        logger.info(f"=== Running Numerical Experiment: {dataset_name} ===")
        
        # Load d·ªØ li·ªáu
        df = self.data_loader.load_numerical_data(dataset_name)
        
        # Ph√¢n t√≠ch dataset
        analysis = self.numerical_processor.analyze_dataset(df)
        
        # T·∫°o correlation matrix
        correlations = self.numerical_processor.create_correlations(df)
        
        # Ph√°t hi·ªán outliers
        outliers = self.numerical_processor.detect_outliers(df)
        
        # L∆∞u k·∫øt qu·∫£
        self.results[f'numerical_experiment_{dataset_name}'] = {
            'dataset_name': dataset_name,
            'analysis': analysis,
            'correlations': correlations.to_dict(),
            'outliers': outliers
        }
        
        logger.info(f"Numerical experiment completed: {dataset_name}")
        return self.results[f'numerical_experiment_{dataset_name}']
    
    def run_image_experiment(self, category='shapes'):
        """Ch·∫°y th√≠ nghi·ªám v·ªõi d·ªØ li·ªáu ·∫£nh"""
        logger.info(f"=== Running Image Experiment: {category} ===")
        
        # Load d·ªØ li·ªáu
        images, filenames = self.data_loader.load_image_data(category)
        
        # Ph√¢n t√≠ch ·∫£nh
        analysis = self.image_processor.analyze_images(images)
        
        # T·∫°o image grid
        self.image_processor.create_image_grid(images, filenames, 
                                             f'outputs/plots/{category}_grid.png')
        
        # L∆∞u k·∫øt qu·∫£
        self.results[f'image_experiment_{category}'] = {
            'category': category,
            'total_images': len(images),
            'filenames': filenames,
            'analysis': analysis
        }
        
        logger.info(f"Image experiment completed: {category}")
        return self.results[f'image_experiment_{category}']
    
    def run_multimodal_experiment(self):
        """Ch·∫°y th√≠ nghi·ªám multimodal"""
        logger.info("=== Running Multimodal Experiment ===")
        
        # Load d·ªØ li·ªáu t·ª´ nhi·ªÅu ngu·ªìn
        text_data = self.data_loader.load_text_data('all')
        sales_data = self.data_loader.load_numerical_data('sales_data')
        images, _ = self.data_loader.load_image_data('shapes')
        
        # T·∫°o features
        text_features = torch.randn(len(text_data['all_samples']), 768)
        numerical_features = torch.randn(len(sales_data), 512)
        image_features = torch.randn(len(images), 256)
        
        # Fusion (ƒë∆°n gi·∫£n)
        min_samples = min(len(text_features), len(numerical_features), len(image_features))
        fused_features = torch.cat([
            text_features[:min_samples],
            numerical_features[:min_samples],
            image_features[:min_samples]
        ], dim=1)
        
        # L∆∞u k·∫øt qu·∫£
        self.results['multimodal_experiment'] = {
            'text_samples': len(text_data['all_samples']),
            'numerical_samples': len(sales_data),
            'image_samples': len(images),
            'fused_samples': min_samples,
            'fused_feature_dim': fused_features.shape[1],
            'feature_shapes': {
                'text': text_features.shape,
                'numerical': numerical_features.shape,
                'image': image_features.shape,
                'fused': fused_features.shape
            }
        }
        
        logger.info(f"Multimodal experiment completed: {min_samples} fused samples")
        return self.results['multimodal_experiment']
    
    def save_results(self, filename='outputs/results/experiment_results.json'):
        """L∆∞u k·∫øt qu·∫£ th√≠ nghi·ªám"""
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ ƒë·ªÉ c√≥ th·ªÉ serialize
        serializable_results = {}
        for key, value in self.results.items():
            serializable_results[key] = self._make_serializable(value)
        
        # Th√™m timestamp
        results_with_timestamp = {
            'timestamp': datetime.now().isoformat(),
            'results': serializable_results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results_with_timestamp, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Results saved to: {filename}")
    
    def _make_serializable(self, obj):
        import pandas as pd
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (torch.Tensor, torch.Size)):
            return obj.tolist() if hasattr(obj, 'tolist') else str(obj)
        elif isinstance(obj, type) and hasattr(obj, '__module__') and 'pandas' in obj.__module__:
            return str(obj)
        elif hasattr(obj, '__module__') and 'pandas' in obj.__module__:
            return str(obj)
        elif hasattr(obj, '__class__') and 'pandas' in str(obj.__class__):
            return str(obj)
        elif isinstance(obj, pd.api.extensions.ExtensionDtype):
            return str(obj)
        elif hasattr(obj, 'dtype'):
            return str(obj)
        else:
            return obj
    
    def generate_report(self):
        """T·∫°o b√°o c√°o t·ªïng h·ª£p"""
        logger.info("=== Generating Report ===")
        
        report = {
            'summary': {
                'total_experiments': len(self.results),
                'experiment_names': list(self.results.keys()),
                'timestamp': datetime.now().isoformat()
            },
            'details': self.results
        }
        
        # L∆∞u b√°o c√°o
        report_path = 'outputs/results/experiment_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Report generated: {report_path}")
        return report

    def save_history_log(self, step_name: str, content_obj):
        """L∆∞u l·∫°i log qu√° tr√¨nh l√†m vi·ªác v√†o folder historylog, lu√¥n l∆∞u log k·ªÉ c·∫£ khi g·∫∑p l·ªói serialization"""
        import json
        os.makedirs('historylog', exist_ok=True)
        filename = f"historylog/{step_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        try:
            if isinstance(content_obj, str):
                content = content_obj
            else:
                try:
                    content = json.dumps(self._make_serializable(content_obj), ensure_ascii=False, indent=2)
                except Exception as e:
                    content = f"[SerializationError] {e}\nRaw str:\n{str(content_obj)}"
        except Exception as e:
            content = f"[CriticalError] {e}\nRaw str:\n{str(content_obj)}"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        logger.info(f"Saved history log: {filename}")

def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y t·∫•t c·∫£ th√≠ nghi·ªám"""
    logger.info("=== Tri·ªÉn khai th·ª≠ nghi·ªám v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø ===")
    runner = ExperimentRunner()
    try:
        # 1. Text experiment
        text_results = runner.run_text_experiment()
        runner.save_history_log('step_01_text_experiment', json.dumps(runner._make_serializable(text_results), ensure_ascii=False, indent=2))

        # 2. Numerical experiments
        numerical_results_1 = runner.run_numerical_experiment('sales_data')
        runner.save_history_log('step_02_numerical_sales', json.dumps(runner._make_serializable(numerical_results_1), ensure_ascii=False, indent=2))
        numerical_results_2 = runner.run_numerical_experiment('user_behavior')
        runner.save_history_log('step_03_numerical_user_behavior', json.dumps(runner._make_serializable(numerical_results_2), ensure_ascii=False, indent=2))
        numerical_results_3 = runner.run_numerical_experiment('sensor_data')
        runner.save_history_log('step_04_numerical_sensor_data', json.dumps(runner._make_serializable(numerical_results_3), ensure_ascii=False, indent=2))

        # 3. Image experiments
        image_results_1 = runner.run_image_experiment('shapes')
        runner.save_history_log('step_05_image_shapes', json.dumps(runner._make_serializable(image_results_1), ensure_ascii=False, indent=2))
        image_results_2 = runner.run_image_experiment('colors')
        runner.save_history_log('step_06_image_colors', json.dumps(runner._make_serializable(image_results_2), ensure_ascii=False, indent=2))

        # 4. Multimodal experiment
        multimodal_results = runner.run_multimodal_experiment()
        runner.save_history_log('step_07_multimodal', json.dumps(runner._make_serializable(multimodal_results), ensure_ascii=False, indent=2))

        # L∆∞u k·∫øt qu·∫£
        runner.save_results()

        # T·∫°o b√°o c√°o
        report = runner.generate_report()
        runner.save_history_log('step_08_report', json.dumps(runner._make_serializable(report), ensure_ascii=False, indent=2))

        # In t√≥m t·∫Øt
        print("\n" + "="*60)
        print("‚úÖ HO√ÄN TH√ÄNH T·∫§T C·∫¢ TH√ç NGHI·ªÜM!")
        print("="*60)
        print(f"üìä T·ªïng s·ªë th√≠ nghi·ªám: {len(runner.results)}")
        print(f"üìù Text samples: {text_results['total_samples']}")
        print(f"üî¢ Numerical datasets: 4")
        print(f"üñºÔ∏è Image categories: 3")
        print(f"üîó Multimodal samples: {multimodal_results['fused_samples']}")
        print(f"üíæ K·∫øt qu·∫£ ƒë√£ l∆∞u: outputs/results/")
        print(f"üìà Bi·ªÉu ƒë·ªì ƒë√£ t·∫°o: outputs/plots/")
    except Exception as e:
        logger.error(f"Error during experiments: {e}")
        import traceback
        tb = traceback.format_exc()
        runner.save_history_log('error', tb)
        print(tb)

if __name__ == "__main__":
    main() 