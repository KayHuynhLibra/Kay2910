# üöÄ Project - 02 Orchestration

## üìã M√¥ T·∫£ D·ª± √Ån / Project Description

**Ti·∫øng Vi·ªát:** X√¢y d·ª±ng m·ªôt h·ªá th·ªëng ho√†n ch·ªânh s·ª≠ d·ª•ng 02 Orchestration ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n th·ª±c t·∫ø.

**English:** Build a complete system using 02 Orchestration to solve a real-world problem.

## üéØ M·ª•c Ti√™u / Objectives

1. **Hi·ªÉu s√¢u v·ªÅ 02 Orchestration** / Deep understanding of 02 Orchestration
2. **Implement t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi** / End-to-end implementation
3. **ƒê√°nh gi√° hi·ªáu su·∫•t** / Performance evaluation
4. **T·ªëi ∆∞u h√≥a model** / Model optimization

## üìä Dataset / B·ªô D·ªØ Li·ªáu

```python
# Sample dataset creation
import pandas as pd
import numpy as np

def create_sample_dataset():
    '''
    Create sample dataset for 02 Orchestration project
    '''
    np.random.seed(42)
    n_samples = 1000
    
    # Generate features
    X = np.random.randn(n_samples, 10)
    
    # Generate target (example for regression)
    y = np.dot(X, np.random.randn(10)) + np.random.randn(n_samples) * 0.1
    
    # Create DataFrame
    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])
    df['target'] = y
    
    return df

# Usage
dataset = create_sample_dataset()
print(f"Dataset shape: {dataset.shape}")
```

## üõ†Ô∏è Implementation Steps / C√°c B∆∞·ªõc Th·ª±c Hi·ªán

### Step 1: Data Preparation / B∆∞·ªõc 1: Chu·∫©n B·ªã D·ªØ Li·ªáu
```python
def prepare_project_data(df):
    '''
    Prepare data for 02 Orchestration project
    '''
    # Handle missing values
    df = df.dropna()
    
    # Feature scaling
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    
    X = df.drop('target', axis=1)
    y = df['target']
    
    X_scaled = scaler.fit_transform(X)
    
    return X_scaled, y, scaler
```

### Step 2: Model Development / B∆∞·ªõc 2: Ph√°t Tri·ªÉn Model
```python
def develop_02_orchestration_model(X, y):
    '''
    Develop 02 Orchestration model for project
    '''
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_squared_error, r2_score
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # Import and train model (example with Linear Regression)
    from sklearn.linear_model import LinearRegression
    model = LinearRegression()
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Evaluate
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f"Model Performance:")
    print(f"MSE: {mse:.4f}")
    print(f"R¬≤: {r2:.4f}")
    
    return model, y_pred
```

### Step 3: Model Optimization / B∆∞·ªõc 3: T·ªëi ∆Øu H√≥a Model
```python
def optimize_02_orchestration_model(X, y):
    '''
    Optimize 02 Orchestration model using hyperparameter tuning
    '''
    from sklearn.model_selection import GridSearchCV
    from sklearn.linear_model import Ridge
    
    # Define parameter grid
    param_grid = {
        'alpha': [0.1, 1.0, 10.0, 100.0],
        'solver': ['auto', 'svd', 'cholesky']
    }
    
    # Grid search
    grid_search = GridSearchCV(
        Ridge(), param_grid, cv=5, scoring='neg_mean_squared_error'
    )
    grid_search.fit(X, y)
    
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best score: {-grid_search.best_score_:.4f}")
    
    return grid_search.best_estimator_
```

## üìà Evaluation / ƒê√°nh Gi√°

### Metrics / C√°c Ch·ªâ S·ªë
- **MSE**: Mean Squared Error
- **RMSE**: Root Mean Squared Error  
- **MAE**: Mean Absolute Error
- **R¬≤**: Coefficient of Determination

### Visualization / Minh H·ªça
```python
def visualize_project_results(y_true, y_pred):
    '''
    Visualize project results
    '''
    import matplotlib.pyplot as plt
    
    plt.figure(figsize=(12, 4))
    
    # Prediction vs Actual
    plt.subplot(1, 2, 1)
    plt.scatter(y_true, y_pred, alpha=0.5)
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title('Prediction vs Actual')
    
    # Residuals
    plt.subplot(1, 2, 2)
    residuals = y_true - y_pred
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.title('Residual Plot')
    
    plt.tight_layout()
    plt.show()
```

## üìù Deliverables / S·∫£n Ph·∫©m

1. **Code ho√†n ch·ªânh** / Complete code
2. **B√°o c√°o k·∫øt qu·∫£** / Results report
3. **Presentation slides** / Slides thuy·∫øt tr√¨nh
4. **Demo video** / Video demo

## üîó Related Links / Li√™n K·∫øt Li√™n Quan

- [Theory](./THEORY_02_orchestration.md)
- [Implementation](./IMPLEMENTATION_02_orchestration.md)
- [Code Examples](./CODE_EXAMPLES_02_orchestration.md)
- [Quiz](./QUIZ_02_orchestration.md)
